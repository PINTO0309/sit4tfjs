#!/usr/bin/env python3
"""
Test script for sit4tfjs benchmarking tool.
"""

import sys
import os
from pathlib import Path

# Add the sit4tfjs package to the Python path
sys.path.insert(0, str(Path(__file__).parent))

from sit4tfjs import TFJSBenchmark, get_model_info


def test_model_info():
    """Test model information loading."""
    print("Testing model information loading...")
    
    # Test with multi-input model
    multi_input_path = "./model_multi_input_fix_tfjs"
    if Path(multi_input_path).exists():
        try:
            model_info = get_model_info(multi_input_path)
            print(f"✓ Successfully loaded model info from {multi_input_path}")
            print(f"  Format: {model_info.get('format', 'unknown')}")
            print(f"  Generated by: {model_info.get('generatedBy', 'unknown')}")
            
            if "signature" in model_info:
                inputs = model_info["signature"]["inputs"]
                outputs = model_info["signature"]["outputs"]
                print(f"  Inputs: {len(inputs)}")
                print(f"  Outputs: {len(outputs)}")
            
        except Exception as e:
            print(f"✗ Failed to load model info: {e}")
            return False
    else:
        print(f"✗ Model directory not found: {multi_input_path}")
        return False
    
    return True


def test_benchmark_multi_input():
    """Test benchmarking with multi-input model."""
    print("\\nTesting multi-input model benchmarking...")
    
    model_path = "./model_multi_input_fix_tfjs"
    if not Path(model_path).exists():
        print(f"✗ Model directory not found: {model_path}")
        return False
    
    try:
        benchmark = TFJSBenchmark(
            model_path=model_path,
            batch_size=1,
            test_loop_count=5,  # Small number for testing
            enable_profiling=True
        )
        
        # Get input specs
        input_specs = benchmark.get_input_specs()
        print(f"✓ Successfully loaded model with {len(input_specs)} inputs")
        for name, spec in input_specs.items():
            print(f"  {name}: {spec['shape']} ({spec['dtype']})")
        
        # Get output specs
        output_specs = benchmark.get_output_specs()
        print(f"✓ Model has {len(output_specs)} outputs")
        for name, spec in output_specs.items():
            print(f"  {name}: {spec['shape']} ({spec['dtype']})")
        
        # Generate inputs
        inputs = benchmark.generate_random_inputs()
        print(f"✓ Generated random inputs")
        
        # Run a single inference
        outputs, inference_time = benchmark.run_inference(inputs)
        print(f"✓ Single inference completed in {inference_time:.2f} ms")
        print(f"  Output shapes: {[list(arr.shape) for arr in outputs.values()]}")
        
        return True
        
    except Exception as e:
        print(f"✗ Benchmark test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_benchmark_optimized():
    """Test benchmarking with optimized model."""
    print("\\nTesting optimized model benchmarking...")
    
    model_path = "./model_optimized_fix_tfjs"
    if not Path(model_path).exists():
        print(f"✗ Model directory not found: {model_path}")
        return False
    
    try:
        benchmark = TFJSBenchmark(
            model_path=model_path,
            batch_size=1,
            test_loop_count=3,  # Small number for testing
            enable_profiling=False
        )
        
        # Get model specs
        input_specs = benchmark.get_input_specs()
        output_specs = benchmark.get_output_specs()
        
        print(f"✓ Optimized model loaded:")
        print(f"  Inputs: {len(input_specs)}")
        print(f"  Outputs: {len(output_specs)}")
        
        # Generate inputs and run inference
        inputs = benchmark.generate_random_inputs()
        outputs, inference_time = benchmark.run_inference(inputs)
        print(f"✓ Inference completed in {inference_time:.2f} ms")
        
        return True
        
    except Exception as e:
        print(f"✗ Optimized model test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run all tests."""
    print("=" * 60)
    print("sit4tfjs Test Suite")
    print("=" * 60)
    
    test_results = []
    
    # Test 1: Model info loading
    test_results.append(test_model_info())
    
    # Test 2: Multi-input model benchmarking
    test_results.append(test_benchmark_multi_input())
    
    # Test 3: Optimized model benchmarking
    test_results.append(test_benchmark_optimized())
    
    # Summary
    passed = sum(test_results)
    total = len(test_results)
    
    print("\\n" + "=" * 60)
    print(f"Test Results: {passed}/{total} tests passed")
    print("=" * 60)
    
    if passed == total:
        print("✓ All tests passed!")
        return 0
    else:
        print("✗ Some tests failed!")
        return 1


if __name__ == "__main__":
    sys.exit(main())